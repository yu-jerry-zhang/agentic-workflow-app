# Universal Product Reconstructor — AI Agentic Workflow

**Course:** 94-844 Generative AI Lab (Fall 2025)  
**Project:** Final Project (Q4 Bonus)

## Overview
A scalable, autonomous AI agentic workflow that discovers, analyzes, and visually reconstructs products from web data. Built with Streamlit for the frontend and OpenAI models (GPT-4o for language understanding and DALL·E 3 for image synthesis).

Key capabilities:
- Multi-agent pipeline for data ingestion, analysis, prompt synthesis, and visualization
- Live scraping mode and cached-data mode for reproducible demos
- Generates structured visual features, sentiment scores, image prompts, and visual prototypes

## System Architecture
Four sequential agents:

1. Researcher Agent
    - Function: Data ingestion
    - Live Mode: Selenium-based scraping of Amazon product pages (descriptions, reviews)
    - Cached Mode: Loads pre-verified JSON datasets from disk

2. Analyst Agent
    - Function: Use LLMs (GPT-4o) to parse unstructured text
    - Output: Extracts objective visual features and computes sentiment analysis

3. Creative Agent
    - Function: Convert analyst output into a high-fidelity image-generation prompt optimized for diffusion models

4. Visualizer Agent
    - Function: Use DALL·E 3 to generate a visual prototype from the creative prompt

## Tech Stack
- Frontend: Streamlit
- LLMs / Images: OpenAI GPT-4o, DALL·E 3
- Scraping: Selenium (ChromeDriver)
- Language: Python

## Project Structure
```text
universal_product_agent/
├── app.py                 # Streamlit dashboard (frontend)
├── agents.py              # Agent definitions and orchestration
├── scraper.py             # Selenium scraper
├── requirements.txt       # Python dependencies
├── .env                   # Environment variables (API keys) — DO NOT COMMIT
├── README.md              # Project documentation
└── data/                  # Cached product datasets
     └── keyboard/
          ├── product_description.json
          └── customer_reviews.json
```

## Setup

1. Create a virtual environment and install dependencies:
    - python -m venv .venv
    - source .venv/bin/activate  (or .venv\Scripts\activate on Windows)
    - pip install -r requirements.txt

2. Configure API keys
    - Create a file named .env in the project root with the following (example):
      OPENAI_API_KEY=sk-REPLACE_WITH_YOUR_KEY
    - Never commit .env to version control

3. ChromeDriver (for live scraping)
    - Install Chrome and the matching ChromeDriver version accessible in PATH

## Launch the Application
Run:
streamlit run app.py

The app will open at http://localhost:8501 by default.

## Usage

Mode A — Live Web Scraping
- Select "Live Web Scraping" in the sidebar
- Enter an Amazon ASIN (e.g., B0CCP8KYGG)
- Click "Start Full Pipeline"
- Note: A Chrome window will open; the pipeline waits ~45s to allow manual login when required

Mode B — Load Existing Data
- Place JSON files under data/{product_name}/
- Select "Load Existing Data" in the sidebar
- Choose a product folder and click "Start Full Pipeline"

## Outputs
- Sentiment Score: 1–10 rating derived from review analysis
- Visual Features: Structured list of objective physical attributes extracted from text
- Image Prompt: Optimized prompt for diffusion-based generators
- Final Image: Visual prototype generated by DALL·E 3

## Notes & Safety
- Protect your API keys; do not push .env to public repos
- Follow OpenAI usage policies for model and image generation
